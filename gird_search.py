import pandas as pd
import jieba
import re
import time
from tqdm import tqdm
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
from scipy.sparse import hstack
import matplotlib.pyplot as plt
import joblib
import os

plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# ------------------------------
# 0. éšæœºæŠ½æ ·å‡½æ•°
# ------------------------------
def sample_csv(input_file, output_file, n, random_state=42):
    df = pd.read_csv(input_file, encoding='utf-8-sig')
    sampled_df = df.sample(n=n, random_state=random_state)
    sampled_df.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"âœ… å·²ä» {input_file} ä¸­éšæœºæŠ½å– {n} è¡Œï¼Œä¿å­˜ä¸º {output_file}")
    return sampled_df

# ------------------------------
# 1. åœç”¨è¯ & æƒ…æ„Ÿè¯å…¸åŠ è½½
# ------------------------------
with open("stopwords.txt", encoding="utf-8") as f:
    stopwords = set([line.strip() for line in f])

positive_words = {'å–œæ¬¢', 'æ»¡æ„', 'æ£’', 'å¥½', 'ä¼˜ç§€', 'å€¼å¾—', 'å¿«ä¹', 'çœŸç†', 'è¿›æ­¥'}
negative_words = {'å·®', 'ç³Ÿç³•', 'å', 'å¤±æœ›', 'ä½çº§', 'å¤±è´¥', 'ä¸€è´¥æ¶‚åœ°'}

# ------------------------------
# 2. æ–‡æœ¬æ¸…æ´—ä¸æƒ…æ„Ÿç‰¹å¾
# ------------------------------
def clean_text(text):
    text = re.sub(r"[^\u4e00-\u9fa5]", " ", text)
    words = jieba.cut(text)
    words = [w for w in words if w.strip() and w not in stopwords]
    return " ".join(words)

def emotion_features(text):
    tokens = text.split()
    pos_count = sum(1 for t in tokens if t in positive_words)
    neg_count = sum(1 for t in tokens if t in negative_words)
    return pd.Series([pos_count, neg_count, pos_count - neg_count])

# ------------------------------
# ä¸»ç¨‹åº
# ------------------------------
def main():
    input_file = "online_shoping.csv"
    sampled_file = "sampled_data.csv"
    sample_size = 5000

    # Step 0: æŠ½æ ·æ•°æ®
    if not os.path.exists(sampled_file):
        df = sample_csv(input_file, sampled_file, n=sample_size)
    else:
        df = pd.read_csv(sampled_file, encoding='utf-8-sig')
        print(f"ğŸ“„ å·²è¯»å–ç°æœ‰æ–‡ä»¶ï¼š{sampled_file}")

    df['review'] = df['review'].fillna('').astype(str)

    print("ğŸš¿ æ­£åœ¨æ¸…æ´—æ–‡æœ¬...")
    tqdm.pandas()
    df["cut_review"] = df["review"].progress_apply(clean_text)

    print("ğŸ” æå–æƒ…æ„Ÿè¯å…¸ç‰¹å¾...")
    df[["pos_count", "neg_count", "emotion_score"]] = df["cut_review"].apply(emotion_features)

    # Step 3: ç‰¹å¾æå–
    print("ğŸ“ æå– TF-IDF ç‰¹å¾...")
    tfidf = TfidfVectorizer(
        max_features=8000,
        min_df=2,
        max_df=0.9,
        ngram_range=(1, 2)
    )
    X_tfidf = tfidf.fit_transform(df['cut_review'])
    X_dict = df[['pos_count', 'neg_count', 'emotion_score']]
    X = hstack([X_tfidf, X_dict.values])
    y = df['label']

    # Step 4: åˆ’åˆ†æ•°æ®é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y)

    # Step 5: æ¨¡å‹è®­ç»ƒä¸ç½‘æ ¼æœç´¢
    print("ğŸ§  è®­ç»ƒæ¨¡å‹ä¸­...")
    start_time = time.time()
    param_grid = {
        'C': [0.5, 1, 5],
        'kernel': ['linear', 'rbf'],
        'gamma': ['scale', 'auto']
    }
    grid = GridSearchCV(SVC(), param_grid, cv=3, n_jobs=-1, verbose=1)
    grid.fit(X_train, y_train)
    end_time = time.time()

    clf = grid.best_estimator_
    print(f"âœ… è®­ç»ƒå®Œæˆï¼è€—æ—¶ {end_time - start_time:.2f} ç§’")
    print("ğŸ“Œ æœ€ä¼˜å‚æ•°ï¼š", grid.best_params_)

    # Step 6: æ¨¡å‹è¯„ä¼°
    y_pred = clf.predict(X_test)
    print(f"\nğŸ¯ å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.4f}")
    print(classification_report(y_test, y_pred, digits=4))

    cm = confusion_matrix(y_test, y_pred)
    ConfusionMatrixDisplay(cm).plot()
    plt.title("æ··æ·†çŸ©é˜µ")
    plt.show()

    # Step 7: æ¨¡å‹ä¿å­˜
    joblib.dump(clf, "svm_model_with_lexicon.pkl")
    joblib.dump(tfidf, "tfidf_vectorizer_with_lexicon.pkl")
    print("ğŸ’¾ æ¨¡å‹å’Œå‘é‡å™¨å·²ä¿å­˜ä¸º 'svm_model_with_lexicon.pkl' å’Œ 'tfidf_vectorizer_with_lexicon.pkl'")

if __name__ == "__main__":
    main()
